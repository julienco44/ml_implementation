{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ames Housing Dataset - Preprocessing\n",
    "\n",
    "This notebook preprocesses the **Ames Housing Dataset** - a real dataset with many features.\n",
    "\n",
    "**Dataset**: Ames Housing (via sklearn/OpenML)  \n",
    "**Samples**: ~1,460  \n",
    "**Features**: 79 (large feature space!)  \n",
    "**Target**: SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.datasets import fetch_openml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset from OpenML\n",
    "\n",
    "The Ames Housing dataset is available directly via sklearn's fetch_openml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ames Housing from OpenML\n",
    "print(\"Downloading Ames Housing dataset...\")\n",
    "ames = fetch_openml(name=\"house_prices\", as_frame=True, parser='auto')\n",
    "\n",
    "df = ames.frame\n",
    "print(f\"Dataset loaded: {df.shape[0]} samples, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path('../data')\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save raw data\n",
    "df.to_csv(DATA_DIR / 'ames_housing_raw.csv', index=False)\n",
    "print(f\"Raw data saved to {DATA_DIR / 'ames_housing_raw.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumn types:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "TARGET = 'SalePrice'\n",
    "\n",
    "print(f\"Target: {TARGET}\")\n",
    "print(f\"  Min: ${df[TARGET].min():,.0f}\")\n",
    "print(f\"  Max: ${df[TARGET].max():,.0f}\")\n",
    "print(f\"  Mean: ${df[TARGET].mean():,.0f}\")\n",
    "print(f\"  Median: ${df[TARGET].median():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].hist(df[TARGET], bins=50, color='steelblue', edgecolor='white')\n",
    "ax[0].set_xlabel('Sale Price ($)')\n",
    "ax[0].set_title('Sale Price Distribution')\n",
    "ax[1].hist(np.log1p(df[TARGET]), bins=50, color='coral', edgecolor='white')\n",
    "ax[1].set_xlabel('Log(Sale Price)')\n",
    "ax[1].set_title('Log-transformed Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(1)\n",
    "missing_df = pd.DataFrame({'Missing': missing, 'Percent': missing_pct})\n",
    "missing_df = missing_df[missing_df['Missing'] > 0].sort_values('Missing', ascending=False)\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_df)}\")\n",
    "print(missing_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature types\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if TARGET in numerical_cols:\n",
    "    numerical_cols.remove(TARGET)\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_cols)}\")\n",
    "print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "print(f\"Total features: {len(numerical_cols) + len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "\n",
    "# Drop columns with too many missing values (>50%)\n",
    "drop_cols = missing_df[missing_df['Percent'] > 50].index.tolist()\n",
    "if drop_cols:\n",
    "    print(f\"Dropping columns with >50% missing: {drop_cols}\")\n",
    "    df_processed = df_processed.drop(columns=drop_cols)\n",
    "\n",
    "# Drop Id column if present\n",
    "if 'Id' in df_processed.columns:\n",
    "    df_processed = df_processed.drop(columns=['Id'])\n",
    "    print(\"Dropped 'Id' column\")\n",
    "\n",
    "print(f\"Shape after dropping: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update column lists\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "if TARGET in numerical_cols:\n",
    "    numerical_cols.remove(TARGET)\n",
    "\n",
    "print(f\"Numerical: {len(numerical_cols)}, Categorical: {len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numerical values with median\n",
    "for col in numerical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        median_val = df_processed[col].median()\n",
    "        df_processed[col] = df_processed[col].fillna(median_val)\n",
    "\n",
    "# Fill missing categorical values with mode or 'None'\n",
    "for col in categorical_cols:\n",
    "    if df_processed[col].isnull().sum() > 0:\n",
    "        df_processed[col] = df_processed[col].fillna('None')\n",
    "\n",
    "print(f\"Missing values after filling: {df_processed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "print(f\"Encoding {len(categorical_cols)} categorical columns...\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # Label encode\n",
    "    df_processed[col] = pd.factorize(df_processed[col])[0]\n",
    "\n",
    "print(f\"Final shape: {df_processed.shape}\")\n",
    "print(f\"All columns numerical: {df_processed.select_dtypes(include=['object']).shape[1] == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check\n",
    "print(f\"\\nFinal Dataset:\")\n",
    "print(f\"  Samples: {df_processed.shape[0]}\")\n",
    "print(f\"  Features: {df_processed.shape[1] - 1}\")\n",
    "print(f\"  Target: {TARGET}\")\n",
    "print(f\"  Missing values: {df_processed.isnull().sum().sum()}\")\n",
    "\n",
    "df_processed.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlations with target\n",
    "corr_with_target = df_processed.corr()[TARGET].drop(TARGET).sort_values(ascending=False)\n",
    "print(\"Top 15 features correlated with SalePrice:\")\n",
    "print(corr_with_target.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation bar plot\n",
    "top_corr = corr_with_target.head(15)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(top_corr)), top_corr.values, color='steelblue')\n",
    "plt.yticks(range(len(top_corr)), top_corr.index)\n",
    "plt.xlabel('Correlation with SalePrice')\n",
    "plt.title('Top 15 Features by Correlation')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "Path('../results/figures').mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig('../results/figures/ames_correlation.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CSV\n",
    "df_processed.to_csv(PROCESSED_DIR / 'ames_housing_processed.csv', index=False)\n",
    "\n",
    "# Save numpy arrays\n",
    "X = df_processed.drop(TARGET, axis=1).values\n",
    "y = df_processed[TARGET].values\n",
    "feature_names = [c for c in df_processed.columns if c != TARGET]\n",
    "\n",
    "np.save(PROCESSED_DIR / 'ames_housing_X.npy', X)\n",
    "np.save(PROCESSED_DIR / 'ames_housing_y.npy', y)\n",
    "np.save(PROCESSED_DIR / 'ames_housing_feature_names.npy', np.array(feature_names))\n",
    "\n",
    "print(f\"Saved:\")\n",
    "print(f\"  X: {X.shape} ({X.shape[1]} features - large feature space!)\")\n",
    "print(f\"  y: {y.shape}\")\n",
    "print(f\"  Features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: Ames Housing\")\n",
    "print(f\"Samples: {X.shape[0]}\")\n",
    "print(f\"Features: {X.shape[1]} (LARGE FEATURE SPACE)\")\n",
    "print(f\"\\nTo use in regression_tree.ipynb:\")\n",
    "print(\"  Change load_dataset to use 'ames_housing'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
