{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOT Temperature Dataset - Preprocessing\n",
    "\n",
    "This notebook preprocesses the IOT Temperature Readings dataset from OpenML.\n",
    "\n",
    "**Dataset:** Temperature Readings from IOT Devices (OpenML ID: 43351)\n",
    "\n",
    "**Preprocessing Steps:**\n",
    "1. Download data from OpenML\n",
    "2. Feature engineering (temporal features from timestamps)\n",
    "3. Train/test split BEFORE preprocessing (NO DATA LEAKAGE)\n",
    "4. Handle missing values with TRAIN statistics only\n",
    "5. One-hot encoding for categorical variables\n",
    "6. Save processed data to `/data/processed/`\n",
    "\n",
    "**Target:** Temperature (°C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Paths\n",
    "DATA_DIR = Path.cwd().parent / 'data'\n",
    "PROCESSED_DIR = DATA_DIR / 'processed'\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "TARGET = 'temp'\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR.resolve()}\")\n",
    "print(f\"Output directory: {PROCESSED_DIR.resolve()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Data from OpenML\n",
    "\n",
    "Note: This requires the `openml` package. Install with: `pip install openml`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check if openml is installed\n",
    "try:\n",
    "    import openml\n",
    "    print(\"✓ OpenML package found\")\n",
    "except ImportError:\n",
    "    print(\"ERROR: openml not installed.\")\n",
    "    print(\"Please run: pip install openml\")\n",
    "    raise\n",
    "\n",
    "# Download dataset\n",
    "print(\"\\nDownloading IOT Temperature data from OpenML (ID: 43351)...\")\n",
    "dataset = openml.datasets.get_dataset(43351)\n",
    "\n",
    "print(f\"Dataset: {dataset.name}\")\n",
    "print(f\"Description: {dataset.description[:200]}...\")\n",
    "print(f\"Target: {dataset.default_target_attribute}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "    target=dataset.default_target_attribute,\n",
    "    dataset_format='dataframe'\n",
    ")\n",
    "\n",
    "# Combine into single dataframe\n",
    "df = X.copy()\n",
    "df[TARGET] = y\n",
    "\n",
    "print(f\"\\nData shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(df[TARGET].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Remove duplicates\n",
    "before = len(df)\n",
    "df = df.drop_duplicates()\n",
    "after = len(df)\n",
    "\n",
    "if before > after:\n",
    "    print(f\"Removed {before - after} duplicate rows\")\n",
    "else:\n",
    "    print(\"No duplicates found\")\n",
    "\n",
    "print(f\"\\nFinal dataset: {df.shape[0]} samples, {df.shape[1]} columns\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "\n",
    "Extract temporal features from the timestamp column."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Parse timestamps\n",
    "if 'noted_date' in df.columns:\n",
    "    print(\"Extracting temporal features from 'noted_date'...\")\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df['noted_date'] = pd.to_datetime(df['noted_date'], errors='coerce')\n",
    "    \n",
    "    # Extract features\n",
    "    df['hour'] = df['noted_date'].dt.hour\n",
    "    df['day_of_week'] = df['noted_date'].dt.dayofweek\n",
    "    df['day_of_month'] = df['noted_date'].dt.day\n",
    "    df['month'] = df['noted_date'].dt.month\n",
    "    \n",
    "    print(\"  ✓ Extracted: hour, day_of_week, day_of_month, month\")\n",
    "    \n",
    "    # Drop original timestamp\n",
    "    df = df.drop('noted_date', axis=1)\n",
    "    print(\"  ✓ Dropped original 'noted_date' column\")\n",
    "\n",
    "print(f\"\\nFeatures after engineering: {df.shape[1]} columns\")\n",
    "print(df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Separate features and target\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Drop target and ID columns\n",
    "cols_to_drop = [TARGET]\n",
    "if 'id' in df.columns:\n",
    "    cols_to_drop.append('id')\n",
    "    print(\"Dropped 'id' column (unique identifier)\")\n",
    "\n",
    "X = df.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "y = df[TARGET].values.astype(np.float64)\n",
    "\n",
    "print(f\"Features: {X.shape[1]} columns\")\n",
    "print(f\"Target: {TARGET} (range: {y.min():.1f}°C - {y.max():.1f}°C)\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Train/Test Split"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Train/test split\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Test: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nSplit ratio: {100*(1-TEST_SIZE):.0f}/{100*TEST_SIZE:.0f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Identify Column Types and Store Train Statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: Store preprocessing statistics (TRAIN only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric: {len(numeric_cols)}, Categorical: {len(categorical_cols)}\")\n",
    "print(f\"\\nNumeric columns: {numeric_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Store preprocessing statistics from TRAIN set only\n",
    "train_stats = {}\n",
    "\n",
    "# Numeric: medians from TRAIN\n",
    "for col in numeric_cols:\n",
    "    train_stats[col] = {'median': X_train[col].median()}\n",
    "\n",
    "# Categorical: modes and categories from TRAIN\n",
    "for col in categorical_cols:\n",
    "    mode_val = X_train[col].mode()\n",
    "    train_stats[col] = {\n",
    "        'mode': mode_val[0] if len(mode_val) > 0 else 'Unknown',\n",
    "        'categories': X_train[col].dropna().unique().tolist()\n",
    "    }\n",
    "\n",
    "print(f\"\\nStored stats for {len(train_stats)} columns (from TRAIN only)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Apply Preprocessing\n",
    "\n",
    "Using TRAIN statistics to transform both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: Apply preprocessing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def preprocess(X_df, stats, num_cols, cat_cols):\n",
    "    \"\"\"Apply preprocessing using pre-computed statistics.\"\"\"\n",
    "    X_out = X_df.copy()\n",
    "    \n",
    "    # Fill missing numeric with TRAIN median\n",
    "    for col in num_cols:\n",
    "        if col in stats:\n",
    "            X_out[col] = X_out[col].fillna(stats[col]['median'])\n",
    "    \n",
    "    # Fill missing categorical with TRAIN mode\n",
    "    # Replace unseen categories with mode\n",
    "    for col in cat_cols:\n",
    "        if col in stats:\n",
    "            mode_val = stats[col]['mode']\n",
    "            X_out[col] = X_out[col].fillna(mode_val)\n",
    "            known = stats[col]['categories']\n",
    "            X_out[col] = X_out[col].apply(lambda x: x if x in known else mode_val)\n",
    "    \n",
    "    return X_out\n",
    "\n",
    "X_train_clean = preprocess(X_train, train_stats, numeric_cols, categorical_cols)\n",
    "X_test_clean = preprocess(X_test, train_stats, numeric_cols, categorical_cols)\n",
    "\n",
    "print(f\"Missing after preprocessing:\")\n",
    "print(f\"  Train: {X_train_clean.isnull().sum().sum()}\")\n",
    "print(f\"  Test: {X_test_clean.isnull().sum().sum()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 5: One-Hot Encoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_train_encoded = pd.get_dummies(\n",
    "    X_train_clean,\n",
    "    columns=categorical_cols,\n",
    "    drop_first=True,\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "X_test_encoded = pd.get_dummies(\n",
    "    X_test_clean,\n",
    "    columns=categorical_cols,\n",
    "    drop_first=True,\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "# Align columns (test might have different dummies)\n",
    "train_cols = X_train_encoded.columns.tolist()\n",
    "\n",
    "for col in train_cols:\n",
    "    if col not in X_test_encoded.columns:\n",
    "        X_test_encoded[col] = 0\n",
    "\n",
    "X_test_encoded = X_test_encoded[train_cols]\n",
    "\n",
    "print(f\"Final shapes:\")\n",
    "print(f\"  Train: {X_train_encoded.shape}\")\n",
    "print(f\"  Test: {X_test_encoded.shape}\")\n",
    "print(f\"\\nFinal features: {list(X_train_encoded.columns)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Convert to NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Convert to numpy arrays\n",
    "X_train_arr = X_train_encoded.values.astype(np.float64)\n",
    "X_test_arr = X_test_encoded.values.astype(np.float64)\n",
    "feature_names = X_train_encoded.columns.tolist()\n",
    "\n",
    "# Combined (for analysis notebooks)\n",
    "X_combined = np.vstack([X_train_arr, X_test_arr])\n",
    "y_combined = np.concatenate([y_train, y_test])\n",
    "\n",
    "print(f\"Array shapes:\")\n",
    "print(f\"  X_train: {X_train_arr.shape}\")\n",
    "print(f\"  X_test: {X_test_arr.shape}\")\n",
    "print(f\"  X_combined: {X_combined.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  y_test: {y_test.shape}\")\n",
    "print(f\"  y_combined: {y_combined.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save combined\n",
    "np.save(PROCESSED_DIR / 'iot_temp_X.npy', X_combined)\n",
    "np.save(PROCESSED_DIR / 'iot_temp_y.npy', y_combined)\n",
    "np.save(PROCESSED_DIR / 'iot_temp_feature_names.npy', np.array(feature_names))\n",
    "\n",
    "# Save train/test separately\n",
    "np.save(PROCESSED_DIR / 'iot_temp_X_train.npy', X_train_arr)\n",
    "np.save(PROCESSED_DIR / 'iot_temp_X_test.npy', X_test_arr)\n",
    "np.save(PROCESSED_DIR / 'iot_temp_y_train.npy', y_train)\n",
    "np.save(PROCESSED_DIR / 'iot_temp_y_test.npy', y_test)\n",
    "\n",
    "print(f\"Saved to {PROCESSED_DIR}:\")\n",
    "print(f\"  Combined: X={X_combined.shape}, y={y_combined.shape}\")\n",
    "print(f\"  Train: X={X_train_arr.shape}\")\n",
    "print(f\"  Test: X={X_test_arr.shape}\")\n",
    "print(f\"  Features: {len(feature_names)}\")\n",
    "\n",
    "# Also save a CSV version for easy inspection\n",
    "csv_path = PROCESSED_DIR / 'iot_temp_processed.csv'\n",
    "df_combined = pd.DataFrame(X_combined, columns=feature_names)\n",
    "df_combined[TARGET] = y_combined\n",
    "df_combined.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"  CSV: {csv_path.name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: IOT Temperature Readings\")\n",
    "print(f\"Total: {X_combined.shape[0]} samples, {X_combined.shape[1]} features\")\n",
    "print(f\"Target: {TARGET} (range: {y_combined.min():.1f}°C - {y_combined.max():.1f}°C)\")\n",
    "print(f\"\\n✓ NO DATA LEAKAGE:\")\n",
    "print(f\"  - Train/test split done BEFORE preprocessing\")\n",
    "print(f\"  - Missing values filled with TRAIN statistics only\")\n",
    "print(f\"  - Categorical encoding based on TRAIN categories\")\n",
    "print(f\"  - Temporal features extracted from timestamps\")\n",
    "print(f\"\\n✓ Files saved to: {PROCESSED_DIR}\")\n",
    "print(f\"\\n✓ Ready for analysis!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Verification - Load and Check Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Verify saved files\n",
    "print(\"Verifying saved files...\\n\")\n",
    "\n",
    "X_train_loaded = np.load(PROCESSED_DIR / 'iot_temp_X_train.npy')\n",
    "X_test_loaded = np.load(PROCESSED_DIR / 'iot_temp_X_test.npy')\n",
    "y_train_loaded = np.load(PROCESSED_DIR / 'iot_temp_y_train.npy')\n",
    "y_test_loaded = np.load(PROCESSED_DIR / 'iot_temp_y_test.npy')\n",
    "features_loaded = np.load(PROCESSED_DIR / 'iot_temp_feature_names.npy', allow_pickle=True)\n",
    "\n",
    "print(\"✓ All files loaded successfully!\")\n",
    "print(f\"\\nLoaded shapes:\")\n",
    "print(f\"  X_train: {X_train_loaded.shape}\")\n",
    "print(f\"  X_test: {X_test_loaded.shape}\")\n",
    "print(f\"  y_train: {y_train_loaded.shape}\")\n",
    "print(f\"  y_test: {y_test_loaded.shape}\")\n",
    "print(f\"  features: {len(features_loaded)}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
